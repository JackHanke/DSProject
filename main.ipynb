{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Classification for Genomic Data\n",
    "The main file for testing of classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import preprocessors\n",
    "from data.preprocess_data import RawData, MinMaxScaledData, StandardScaledData\n",
    "# import third party various libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Literal # dan and his typing\n",
    "from collections import defaultdict\n",
    "# small army of sklearn imports\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import RFECV, VarianceThreshold, SelectKBest, f_classif\n",
    "# import specific models \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# stop warning me\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit_and_val_classifier` function is a general function for testing an `sklearn` classifier on the preprocessed dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "raw_data = RawData()\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = raw_data.get_data()\n",
    "\n",
    "def fit_and_val_sklearn_classifier(classifier, X_train=X_train, X_val=X_val, verbose=False):\n",
    "    start = time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    if verbose: print(f'Classifier fit in {(time()-start):.3f}s')\n",
    "    y_pred_val = classifier.predict(X_val)\n",
    "    val_acc = accuracy_score(y_true=y_val, y_pred=y_pred_val)\n",
    "    if verbose: \n",
    "        print(f'Validation accuracy: {(val_acc*100):.3f}%\\nClassification Report (Validation):\\n{classification_report(y_val, y_pred_val)}')\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table of Results\n",
    "*Note that the below cell may take a while to run, as it fits a large number of models*\n",
    "\n",
    "We next generate a summary of how different models, hyperparameters, and preprocessing methods affect the validation accuracy. We use this table to decide that the best combination for modeling is TODO. In general, we see high performance accross many types of tree-based and linear models. \n",
    "\n",
    "The rest of the file shows specifics for different models and  data-preprocessing methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Raw Data</th>\n",
       "      <th>MinMax Scaled Data</th>\n",
       "      <th>Standard Scaled Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(random_state=42)</td>\n",
       "      <td>96.88</td>\n",
       "      <td>96.88</td>\n",
       "      <td>96.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier(random_state=42)</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoostClassifier(n_estimators=10, random_sta...</td>\n",
       "      <td>80.62</td>\n",
       "      <td>80.62</td>\n",
       "      <td>80.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier(n_estimators=5, ran...</td>\n",
       "      <td>98.75</td>\n",
       "      <td>98.75</td>\n",
       "      <td>98.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(random_state=42)</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Models  Raw Data  \\\n",
       "0            DecisionTreeClassifier(random_state=42)     96.88   \n",
       "1            RandomForestClassifier(random_state=42)    100.00   \n",
       "2  AdaBoostClassifier(n_estimators=10, random_sta...     80.62   \n",
       "3  GradientBoostingClassifier(n_estimators=5, ran...     98.75   \n",
       "4                LogisticRegression(random_state=42)    100.00   \n",
       "\n",
       "   MinMax Scaled Data  Standard Scaled Data  \n",
       "0               96.88                 96.88  \n",
       "1              100.00                100.00  \n",
       "2               80.62                 80.62  \n",
       "3               98.75                 98.75  \n",
       "4              100.00                100.00  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def make_val_summary_table(models_lst, preprocessed_data_lst):\n",
    "    df_dict = {'Models':[]}\n",
    "    for index, data_obj in enumerate(preprocessed_data_lst):\n",
    "        data_source = data_obj.get_data()\n",
    "        # ds_name = data_source.name\n",
    "        ds_name = data_obj.name\n",
    "        df_dict[ds_name] = []\n",
    "        for model in models_lst:\n",
    "            if index == 0: df_dict['Models'].append(str(model))\n",
    "            val_acc = fit_and_val_sklearn_classifier(\n",
    "                classifier=model,\n",
    "                X_train=data_source[0],\n",
    "                X_val=data_source[2],\n",
    "            )\n",
    "            df_dict[ds_name].append(val_acc*100)\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    return df\n",
    "\n",
    "rand_state = 42 # \n",
    "summary_table = make_val_summary_table(\n",
    "    models_lst = [\n",
    "        DecisionTreeClassifier(random_state=rand_state),\n",
    "        RandomForestClassifier(random_state=rand_state),\n",
    "        AdaBoostClassifier(n_estimators=10, random_state=rand_state),\n",
    "        GradientBoostingClassifier(n_estimators=5, random_state=rand_state),\n",
    "        LogisticRegression(random_state=rand_state),\n",
    "    ],\n",
    "    preprocessed_data_lst = [\n",
    "        RawData(),\n",
    "        MinMaxScaledData(),\n",
    "        StandardScaledData(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pd.set_option('display.precision', 2) # set the precision of the accuracy in the table\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Values - Nicole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print('--- Random Forest ---')\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "def shapley_value_reduce(model, k=1000):\n",
    "    # Calculate Shapley values\n",
    "    explainer = shap.TreeExplainer(model)  \n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "    # mean absolute Shapley value for each feature \n",
    "    shap_values_mean = np.abs(shap_values).mean(axis=0)  \n",
    "\n",
    "    top_k_features = np.argsort(shap_values_mean)[-k:]  \n",
    "    selected_features = X_train.columns[top_k_features.flatten()]\n",
    "\n",
    "    X_train_reduced = X_train[selected_features]\n",
    "    X_val_reduced = X_val[selected_features] \n",
    "    X_test_reduced = X_test[selected_features]\n",
    "\n",
    "    return X_train_reduced, X_val_reduced, X_test_reduced\n",
    "\n",
    "# model from previous cell\n",
    "X_train_reduced, X_val_reduced, X_test_reduced = shapley_value_reduce(model)\n",
    "\n",
    "model_reduced = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "val_acc = fit_and_val_sklearn_classifier(\n",
    "    classifier = model_reduced,\n",
    "    X_train = X_train_reduced,\n",
    "    X_val = X_val_reduced,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest w/ ANOVA - Nicole\n",
    "TODO explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_feature_performance():\n",
    "    y_train = y_train.values.ravel() if hasattr(y_train, 'values') else y_train.ravel()\n",
    "    y_val = y_val.values.ravel() if hasattr(y_val, 'values') else y_val.ravel()\n",
    "    y_test = y_test.values.ravel() if hasattr(y_test, 'values') else y_test.ravel()\n",
    "\n",
    "    # remove low variance features\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    X_train_var = variance_selector.fit_transform(X_train)\n",
    "    X_val_var = variance_selector.transform(X_val)\n",
    "    X_test_var = variance_selector.transform(X_test)\n",
    "\n",
    "    # use SelectKBest to reduce to top 5000 features based on ANOVA F-statistic\n",
    "    k_best_selector = SelectKBest(f_classif, k=5000)\n",
    "    X_train_kbest = k_best_selector.fit_transform(X_train_var, y_train)\n",
    "    X_val_kbest = k_best_selector.transform(X_val_var)\n",
    "    X_test_kbest = k_best_selector.transform(X_test_var)\n",
    "\n",
    "    base_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    base_model.fit(X_train_kbest, y_train)\n",
    "\n",
    "    # get top 1000 features based on feature importance\n",
    "    importances = base_model.feature_importances_\n",
    "    top_indices = np.argsort(importances)[-1000:]\n",
    "\n",
    "    X_train_final = X_train_kbest[:, top_indices]\n",
    "    X_val_final = X_val_kbest[:, top_indices]\n",
    "    X_test_final = X_test_kbest[:, top_indices]\n",
    "    \n",
    "    return X_train_final, X_val_final, X_test_final\n",
    "\n",
    "# train model on reduced dataset\n",
    "final_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "fit_and_val_sklearn_classifier(\n",
    "    classifier=final_model,\n",
    "    X_train = X_train_final,\n",
    "    X_val = X_val_final,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# test model on test set\n",
    "y_pred_test = final_model.predict(X_test_final)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Classification Report (Test):\\n\", classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Regression - Hanna\n",
    "TODO explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#initialize logistic regression model, configured for multinomial classificaiton (>2 classes), using 'lbfgs' solver\n",
    "#the 'max_iter' parameter ensures sufficient iterations for convergence, and 'random_state' ensures reproducibility\n",
    "#https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "\n",
    "print(f'--- Logistic Regression ---')\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=log_reg, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means with PCA - Hanna\n",
    "TODO explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#load data; load the feature data and labels from Parquet files into Pandas DataFrames\n",
    "#the 'data.parquet' file contains the features, while 'labels.parquet' contains the target class labels\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html\n",
    "data = pd.read_parquet('data.parquet')\n",
    "labels = pd.read_parquet('labels.parquet')\n",
    "\n",
    "#map the labels to an encoding dicitonary to ensure consistent class mapping\n",
    "labels['Class'] = labels['Class'].map(encoding_dict)\n",
    "\n",
    "#rename columns for readability and usability\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html\n",
    "data.rename(columns={'Unnamed: 0': 'sample_id'}, inplace=True)\n",
    "labels.rename(columns={'Unnamed: 0': 'sample_id'}, inplace=True)\n",
    "\n",
    "\n",
    "#merge datasets; combine the features and labels intoa single DataFrame, joining on the common column 'sample_id'\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\n",
    "merged_data = pd.merge(data, labels, on='sample_id')\n",
    "\n",
    "#drop the sample_id column as it isn't needed for analaysis\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "#features\n",
    "data = merged_data.drop(columns=['sample_id', 'Class'])\n",
    "#labels\n",
    "labels = merged_data['Class']\n",
    "\n",
    "#standardize features with StandardScalar to normalize values, ensuring each feature has a mean of 0 and\n",
    "# a standard deviation of 1\n",
    "#https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "scaler = StandardScaler()\n",
    "#fit and transform the training data\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Grid Search CV: https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/\n",
    "def pca_grid_search(data, components_range):\n",
    "    results = {}\n",
    "    for n in components_range:\n",
    "        pca = PCA(n_components=n, random_state=42)\n",
    "        reduced_data = pca.fit_transform(data)\n",
    "        variance_explained = sum(pca.explained_variance_ratio_)\n",
    "        results[n] = variance_explained\n",
    "    return results\n",
    "\n",
    "#perform PCA grid search\n",
    "#components_range = rcomponents_range = range(200, min(data_scaled.shape[1], 801), 50) # Include up to 801 or total features - 2 minutes and 6.4 seconds\n",
    "max_components = min(500, data_scaled.shape[1]) # Limit to 500 components or feature count\n",
    "components_range = range(50, max_components, 50) # Start from 50 with a step of 50\n",
    "variance_results = pca_grid_search(data_scaled, components_range)\n",
    "\n",
    "#find the optimal number of components (e.g., retain ~95% variance)\n",
    "optimal_components = max(k for k, v in variance_results.items() if v >= 0.85)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#apply PCA for dimensionality reduction: reduce the feature set to 20 principlal components to simplify the dataset\n",
    "# while retraining as much variance as possible\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "pca = PCA(n_components=optimal_components, random_state=42)\n",
    "data_reduced = pca.fit_transform(data_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#apply k-means clustering\n",
    "#https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html\n",
    "#define the number of clustesr\n",
    "clusters = len(np.unique(labels))\n",
    "#apply kmeans\n",
    "k_means = KMeans(n_clusters=clusters, random_state=42, init='k-means++')\n",
    "#fit the scaled down data\n",
    "k_means.fit(data_reduced)\n",
    "y_kmeans = k_means.predict(data_reduced)\n",
    "\n",
    "#get clusters and centroids\n",
    "cluster_labels = k_means.labels_\n",
    "centroids = k_means.cluster_centers_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #compute the contingency matrix\n",
    "# #https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.cluster.contingency_matrix.html\n",
    "# cont_matrix = contingency_matrix(labels, cluster_labels)\n",
    "\n",
    "# #match each cluster to the true label with the highest count\n",
    "# cluster_label_match = np.argmax(cont_matrix, axis=1)\n",
    "\n",
    "# #map predicted cluster labels to the corresponding true labels\n",
    "# mapped_cluster_labels = np.array([cluster_label_match[label] for label in cluster_labels])\n",
    "\n",
    "\n",
    "\n",
    "#compute the contingency matrix\n",
    "cont_matrix = contingency_matrix(labels, cluster_labels)\n",
    "\n",
    "#solve the optimal alignment problem\n",
    "row_ind, col_ind = linear_sum_assignment(-cont_matrix) # Negative for maximization\n",
    "\n",
    "#map cluster labels to ground truth labels\n",
    "mapped_cluster_labels = np.zeros_like(cluster_labels)\n",
    "for i, j in zip(row_ind, col_ind):\n",
    "    mapped_cluster_labels[cluster_labels == j] = i\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#accuracy and classificaition report\n",
    "accuracy= accuracy_score(labels, mapped_cluster_labels)\n",
    "classification_r = classification_report(labels, mapped_cluster_labels)\n",
    "\n",
    "print(\"K-Means Report\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report (Test):\\n\", classification_r)\n",
    "\n",
    "#silhouette score with euclidean metric\n",
    "# silhouette_s = silhouette_score(X=data_scaled, labels=cluster_labels, metric='euclidean')\n",
    "# print(f\"Silhouette Score: {silhouette_s:.2f}\")\n",
    "\n",
    "#2d visualization using PCA (reduce to 2 dimensions for plotting)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "x=data_reduced[:, 0], y=data_reduced[:, 1], hue=mapped_cluster_labels, palette='viridis', s=50, alpha=0.8)\n",
    "plt.scatter(k_means.cluster_centers_[:, 0], k_means.cluster_centers_[:, 1], c='red', s=200, alpha=0.8, label=\"Centroids\")\n",
    "plt.title(\"KMeans Clusters with PCA\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\", loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees, AdaBoost & Gradient Boosting - Jack & Dan\n",
    "We next test the boosting algorithms Gradient Boosting and AdaBoost. We conduct a small hyperparameter search over the number of estimators `n_estimators` trained by these ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "adaboost = AdaBoostClassifier(n_estimators=10)\n",
    "\n",
    "gradboost = GradientBoostingClassifier(n_estimators=5)\n",
    "\n",
    "rforest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print('--- Decision Tree ---')\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=dtree, verbose=True)\n",
    "\n",
    "print('--- AdaBoost ---')\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=adaboost, verbose=True)\n",
    "\n",
    "print('--- Gradient Boosting ---')\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=gradboost, verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of performance, both the Gradient Boosting and AdaBoost classifiers achieve >99% validation accuracy and >94% test accuracy on the dataset, though there are some differences in the models. Gradient Boosting achieves this performance reliably, but training is an order of magnitude slower. In contrast, AdaBoost is less reliable, but trains significantly faster even with a larger number of estimators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Hanna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#initalize the SVM model\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "print(\"Support Vector Machine\")\n",
    "#evaluate the svm model\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=svm_model, verbose=True)\n",
    "\n",
    "#test the model on the test set\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_test = svm_model.predict(X_test)\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Classification Report (Test): \\n\", classification_report(y_test, y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msai339",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
