{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Classification for Genomic Data\n",
    "The main file for testing of classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import preprocessors\n",
    "from data.preprocess_data import RawData, MinMaxScaledData, StandardScaledData\n",
    "# import third party various libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Literal # dan and his typing\n",
    "from collections import defaultdict\n",
    "# small army of sklearn imports\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import RFECV, VarianceThreshold, SelectKBest, f_classif\n",
    "# import specific models \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# stop warning me\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit_and_val_classifier` function is a general function for testing an `sklearn` classifier on the preprocessed dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "raw_data = RawData()\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = raw_data.get_data()\n",
    "\n",
    "def fit_and_val_sklearn_classifier(classifier, X_train=X_train, X_val=X_val, verbose=False):\n",
    "    start = time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    if verbose: print(f'Classifier fit in {(time()-start):.3f}s')\n",
    "    y_pred_val = classifier.predict(X_val)\n",
    "    val_acc = accuracy_score(y_true=y_val, y_pred=y_pred_val)\n",
    "    if verbose: \n",
    "        print(f'Validation accuracy: {(val_acc*100):.3f}%\\nClassification Report (Validation):\\n{classification_report(y_val, y_pred_val)}')\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table of Results\n",
    "*Note that the below cell may take a while to run, as it fits a large number of models*\n",
    "\n",
    "We next generate a summary of how different models, hyperparameters, and preprocessing methods affect the validation accuracy. We use this table to decide that the best combination for modeling is TODO. In general, we see high performance accross many types of tree-based and linear models. \n",
    "\n",
    "The rest of the file shows specifics for different models and  data-preprocessing methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Raw Data</th>\n",
       "      <th>MinMax Scaled Data</th>\n",
       "      <th>Standard Scaled Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier(random_state=42)</td>\n",
       "      <td>96.88</td>\n",
       "      <td>96.88</td>\n",
       "      <td>96.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier(random_state=42)</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoostClassifier(n_estimators=10, random_sta...</td>\n",
       "      <td>80.62</td>\n",
       "      <td>80.62</td>\n",
       "      <td>80.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier(n_estimators=5, ran...</td>\n",
       "      <td>98.75</td>\n",
       "      <td>98.75</td>\n",
       "      <td>98.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(random_state=42)</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Models  Raw Data  \\\n",
       "0            DecisionTreeClassifier(random_state=42)     96.88   \n",
       "1            RandomForestClassifier(random_state=42)    100.00   \n",
       "2  AdaBoostClassifier(n_estimators=10, random_sta...     80.62   \n",
       "3  GradientBoostingClassifier(n_estimators=5, ran...     98.75   \n",
       "4                LogisticRegression(random_state=42)    100.00   \n",
       "\n",
       "   MinMax Scaled Data  Standard Scaled Data  \n",
       "0               96.88                 96.88  \n",
       "1              100.00                100.00  \n",
       "2               80.62                 80.62  \n",
       "3               98.75                 98.75  \n",
       "4              100.00                100.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def make_val_summary_table(models_lst, preprocessed_data_lst):\n",
    "    df_dict = {'Models':[]}\n",
    "    for index, data_obj in enumerate(preprocessed_data_lst):\n",
    "        data_source = data_obj.get_data()\n",
    "        # ds_name = data_source.name\n",
    "        ds_name = data_obj.name\n",
    "        df_dict[ds_name] = []\n",
    "        for model in models_lst:\n",
    "            if index == 0: df_dict['Models'].append(str(model))\n",
    "            val_acc = fit_and_val_sklearn_classifier(\n",
    "                classifier=model,\n",
    "                X_train=data_source[0],\n",
    "                X_val=data_source[2],\n",
    "            )\n",
    "            df_dict[ds_name].append(val_acc*100)\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    return df\n",
    "\n",
    "rand_state = 42 # \n",
    "summary_table = make_val_summary_table(\n",
    "    models_lst = [\n",
    "        DecisionTreeClassifier(random_state=rand_state),\n",
    "        RandomForestClassifier(random_state=rand_state),\n",
    "        AdaBoostClassifier(n_estimators=10, random_state=rand_state),\n",
    "        GradientBoostingClassifier(n_estimators=5, random_state=rand_state),\n",
    "        LogisticRegression(random_state=rand_state),\n",
    "    ],\n",
    "    preprocessed_data_lst = [\n",
    "        RawData(),\n",
    "        MinMaxScaledData(),\n",
    "        StandardScaledData(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pd.set_option('display.precision', 2) # set the precision of the accuracy in the table\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Values - Nicole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Random Forest ---\n",
      "Classifier fit in 1.547s\n",
      "Validation accuracy: 100.000%\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       1.00      1.00      1.00        61\n",
      "        COAD       1.00      1.00      1.00        19\n",
      "        KIRC       1.00      1.00      1.00        30\n",
      "        LUAD       1.00      1.00      1.00        19\n",
      "        PRAD       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           1.00       160\n",
      "   macro avg       1.00      1.00      1.00       160\n",
      "weighted avg       1.00      1.00      1.00       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print('--- Random Forest ---')\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier fit in 1.231s\n",
      "Validation accuracy: 56.250%\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       0.64      0.64      0.64        61\n",
      "        COAD       0.75      0.63      0.69        19\n",
      "        KIRC       0.75      0.70      0.72        30\n",
      "        LUAD       0.20      0.16      0.18        19\n",
      "        PRAD       0.38      0.48      0.42        31\n",
      "\n",
      "    accuracy                           0.56       160\n",
      "   macro avg       0.54      0.52      0.53       160\n",
      "weighted avg       0.57      0.56      0.56       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "def shapley_value_reduce(model, k=1000):\n",
    "    # Calculate Shapley values\n",
    "    explainer = shap.TreeExplainer(model)  \n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "    # mean absolute Shapley value for each feature \n",
    "    shap_values_mean = np.abs(shap_values).mean(axis=0)  \n",
    "\n",
    "    top_k_features = np.argsort(shap_values_mean)[-k:]  \n",
    "    selected_features = X_train.columns[top_k_features.flatten()]\n",
    "\n",
    "    X_train_reduced = X_train[selected_features]\n",
    "    X_val_reduced = X_val[selected_features] \n",
    "    X_test_reduced = X_test[selected_features]\n",
    "\n",
    "    return X_train_reduced, X_val_reduced, X_test_reduced\n",
    "\n",
    "# model from previous cell\n",
    "X_train_reduced, X_val_reduced, X_test_reduced = shapley_value_reduce(model)\n",
    "\n",
    "model_reduced = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "val_acc = fit_and_val_sklearn_classifier(\n",
    "    classifier = model_reduced,\n",
    "    X_train = X_train_reduced,\n",
    "    X_val = X_val_reduced,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest w/ ANOVA - Nicole\n",
    "TODO explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier fit in 0.109s\n",
      "Validation accuracy: 100.000%\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       1.00      1.00      1.00        61\n",
      "        COAD       1.00      1.00      1.00        19\n",
      "        KIRC       1.00      1.00      1.00        30\n",
      "        LUAD       1.00      1.00      1.00        19\n",
      "        PRAD       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           1.00       160\n",
      "   macro avg       1.00      1.00      1.00       160\n",
      "weighted avg       1.00      1.00      1.00       160\n",
      "\n",
      "Test Accuracy: 0.9937888198757764\n",
      "Classification Report (Test):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       0.98      1.00      0.99        62\n",
      "        COAD       1.00      1.00      1.00        15\n",
      "        KIRC       1.00      0.97      0.98        30\n",
      "        LUAD       1.00      1.00      1.00        32\n",
      "        PRAD       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           0.99       161\n",
      "   macro avg       1.00      0.99      1.00       161\n",
      "weighted avg       0.99      0.99      0.99       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_by_feature_performance(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    y_train = y_train.values.ravel() if hasattr(y_train, 'values') else y_train.ravel()\n",
    "    y_val = y_val.values.ravel() if hasattr(y_val, 'values') else y_val.ravel()\n",
    "    y_test = y_test.values.ravel() if hasattr(y_test, 'values') else y_test.ravel()\n",
    "\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    X_train_var = variance_selector.fit_transform(X_train)\n",
    "    X_val_var = variance_selector.transform(X_val)\n",
    "    X_test_var = variance_selector.transform(X_test)\n",
    "\n",
    "    k_best_selector = SelectKBest(f_classif, k=5000)\n",
    "    X_train_kbest = k_best_selector.fit_transform(X_train_var, y_train)\n",
    "    X_val_kbest = k_best_selector.transform(X_val_var)\n",
    "    X_test_kbest = k_best_selector.transform(X_test_var)\n",
    "\n",
    "    base_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    base_model.fit(X_train_kbest, y_train)\n",
    "\n",
    "    importances = base_model.feature_importances_\n",
    "    top_indices = np.argsort(importances)[-1000:]\n",
    "\n",
    "    X_train_final = X_train_kbest[:, top_indices]\n",
    "    X_val_final = X_val_kbest[:, top_indices]\n",
    "    X_test_final = X_test_kbest[:, top_indices]\n",
    "    \n",
    "    return X_train_final, X_val_final, X_test_final\n",
    "\n",
    "X_train_final, X_val_final, X_test_final = filter_by_feature_performance(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test\n",
    ")\n",
    "\n",
    "final_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "fit_and_val_sklearn_classifier(\n",
    "    classifier=final_model,\n",
    "    X_train=X_train_final,\n",
    "    X_val=X_val_final,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "y_pred_test = final_model.predict(X_test_final)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Classification Report (Test):\\n\", classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Regression - Hanna\n",
    "TODO explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression ---\n",
      "Classifier fit in 1.432s\n",
      "Validation accuracy: 100.000%\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       1.00      1.00      1.00        61\n",
      "        COAD       1.00      1.00      1.00        19\n",
      "        KIRC       1.00      1.00      1.00        30\n",
      "        LUAD       1.00      1.00      1.00        19\n",
      "        PRAD       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           1.00       160\n",
      "   macro avg       1.00      1.00      1.00       160\n",
      "weighted avg       1.00      1.00      1.00       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#initialize logistic regression model, configured for multinomial classificaiton (>2 classes), using 'lbfgs' solver\n",
    "#the 'max_iter' parameter ensures sufficient iterations for convergence, and 'random_state' ensures reproducibility\n",
    "#https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "\n",
    "print(f'--- Logistic Regression ---')\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=log_reg, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means with PCA - Hanna\n",
    "TODO explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#load data; load the feature data and labels from Parquet files into Pandas DataFrames\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#the 'data.parquet' file contains the features, while 'labels.parquet' contains the target class labels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#map the labels to an encoding dicitonary to ensure consistent class mapping\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    668\u001b[0m     path,\n\u001b[1;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[1;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    676\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    268\u001b[0m     path,\n\u001b[1;32m    269\u001b[0m     filesystem,\n\u001b[1;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m    141\u001b[0m         path_or_handle, mode, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.parquet'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#load data; load the feature data and labels from Parquet files into Pandas DataFrames\n",
    "#the 'data.parquet' file contains the features, while 'labels.parquet' contains the target class labels\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html\n",
    "data = pd.read_parquet('data.parquet')\n",
    "labels = pd.read_parquet('labels.parquet')\n",
    "\n",
    "#map the labels to an encoding dicitonary to ensure consistent class mapping\n",
    "labels['Class'] = labels['Class'].map(encoding_dict)\n",
    "\n",
    "#rename columns for readability and usability\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html\n",
    "data.rename(columns={'Unnamed: 0': 'sample_id'}, inplace=True)\n",
    "labels.rename(columns={'Unnamed: 0': 'sample_id'}, inplace=True)\n",
    "\n",
    "\n",
    "#merge datasets; combine the features and labels intoa single DataFrame, joining on the common column 'sample_id'\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\n",
    "merged_data = pd.merge(data, labels, on='sample_id')\n",
    "\n",
    "#drop the sample_id column as it isn't needed for analaysis\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "#features\n",
    "data = merged_data.drop(columns=['sample_id', 'Class'])\n",
    "#labels\n",
    "labels = merged_data['Class']\n",
    "\n",
    "#standardize features with StandardScalar to normalize values, ensuring each feature has a mean of 0 and\n",
    "# a standard deviation of 1\n",
    "#https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "scaler = StandardScaler()\n",
    "#fit and transform the training data\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Grid Search CV: https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/\n",
    "def pca_grid_search(data, components_range):\n",
    "    results = {}\n",
    "    for n in components_range:\n",
    "        pca = PCA(n_components=n, random_state=42)\n",
    "        reduced_data = pca.fit_transform(data)\n",
    "        variance_explained = sum(pca.explained_variance_ratio_)\n",
    "        results[n] = variance_explained\n",
    "    return results\n",
    "\n",
    "#perform PCA grid search\n",
    "#components_range = rcomponents_range = range(200, min(data_scaled.shape[1], 801), 50) # Include up to 801 or total features - 2 minutes and 6.4 seconds\n",
    "max_components = min(500, data_scaled.shape[1]) # Limit to 500 components or feature count\n",
    "components_range = range(50, max_components, 50) # Start from 50 with a step of 50\n",
    "variance_results = pca_grid_search(data_scaled, components_range)\n",
    "\n",
    "#find the optimal number of components (e.g., retain ~95% variance)\n",
    "optimal_components = max(k for k, v in variance_results.items() if v >= 0.85)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#apply PCA for dimensionality reduction: reduce the feature set to 20 principlal components to simplify the dataset\n",
    "# while retraining as much variance as possible\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "pca = PCA(n_components=optimal_components, random_state=42)\n",
    "data_reduced = pca.fit_transform(data_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#apply k-means clustering\n",
    "#https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html\n",
    "#define the number of clustesr\n",
    "clusters = len(np.unique(labels))\n",
    "#apply kmeans\n",
    "k_means = KMeans(n_clusters=clusters, random_state=42, init='k-means++')\n",
    "#fit the scaled down data\n",
    "k_means.fit(data_reduced)\n",
    "y_kmeans = k_means.predict(data_reduced)\n",
    "\n",
    "#get clusters and centroids\n",
    "cluster_labels = k_means.labels_\n",
    "centroids = k_means.cluster_centers_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #compute the contingency matrix\n",
    "# #https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.cluster.contingency_matrix.html\n",
    "# cont_matrix = contingency_matrix(labels, cluster_labels)\n",
    "\n",
    "# #match each cluster to the true label with the highest count\n",
    "# cluster_label_match = np.argmax(cont_matrix, axis=1)\n",
    "\n",
    "# #map predicted cluster labels to the corresponding true labels\n",
    "# mapped_cluster_labels = np.array([cluster_label_match[label] for label in cluster_labels])\n",
    "\n",
    "\n",
    "\n",
    "#compute the contingency matrix\n",
    "cont_matrix = contingency_matrix(labels, cluster_labels)\n",
    "\n",
    "#solve the optimal alignment problem\n",
    "row_ind, col_ind = linear_sum_assignment(-cont_matrix) # Negative for maximization\n",
    "\n",
    "#map cluster labels to ground truth labels\n",
    "mapped_cluster_labels = np.zeros_like(cluster_labels)\n",
    "for i, j in zip(row_ind, col_ind):\n",
    "    mapped_cluster_labels[cluster_labels == j] = i\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#accuracy and classificaition report\n",
    "accuracy= accuracy_score(labels, mapped_cluster_labels)\n",
    "classification_r = classification_report(labels, mapped_cluster_labels)\n",
    "\n",
    "print(\"K-Means Report\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report (Test):\\n\", classification_r)\n",
    "\n",
    "#silhouette score with euclidean metric\n",
    "# silhouette_s = silhouette_score(X=data_scaled, labels=cluster_labels, metric='euclidean')\n",
    "# print(f\"Silhouette Score: {silhouette_s:.2f}\")\n",
    "\n",
    "#2d visualization using PCA (reduce to 2 dimensions for plotting)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "x=data_reduced[:, 0], y=data_reduced[:, 1], hue=mapped_cluster_labels, palette='viridis', s=50, alpha=0.8)\n",
    "plt.scatter(k_means.cluster_centers_[:, 0], k_means.cluster_centers_[:, 1], c='red', s=200, alpha=0.8, label=\"Centroids\")\n",
    "plt.title(\"KMeans Clusters with PCA\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\", loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees, AdaBoost & Gradient Boosting - Jack & Dan\n",
    "We next test the boosting algorithms Gradient Boosting and AdaBoost. We conduct a small hyperparameter search over the number of estimators `n_estimators` trained by these ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree ---\n",
      "Classifier fit in 2.179s\n",
      "Validation accuracy: 98.125%\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       0.98      0.98      0.98        61\n",
      "        COAD       1.00      0.95      0.97        19\n",
      "        KIRC       0.97      1.00      0.98        30\n",
      "        LUAD       0.95      1.00      0.97        19\n",
      "        PRAD       1.00      0.97      0.98        31\n",
      "\n",
      "    accuracy                           0.98       160\n",
      "   macro avg       0.98      0.98      0.98       160\n",
      "weighted avg       0.98      0.98      0.98       160\n",
      "\n",
      "--- AdaBoost ---\n",
      "Classifier fit in 7.897s\n",
      "Validation accuracy: 80.625%\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       1.00      0.98      0.99        61\n",
      "        COAD       1.00      1.00      1.00        19\n",
      "        KIRC       0.00      0.00      0.00        30\n",
      "        LUAD       0.38      1.00      0.55        19\n",
      "        PRAD       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.81       160\n",
      "   macro avg       0.68      0.80      0.71       160\n",
      "weighted avg       0.74      0.81      0.76       160\n",
      "\n",
      "--- Gradient Boosting ---\n",
      "Classifier fit in 43.121s\n",
      "Validation accuracy: 98.125%\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       0.98      1.00      0.99        61\n",
      "        COAD       1.00      0.89      0.94        19\n",
      "        KIRC       0.97      1.00      0.98        30\n",
      "        LUAD       0.95      1.00      0.97        19\n",
      "        PRAD       1.00      0.97      0.98        31\n",
      "\n",
      "    accuracy                           0.98       160\n",
      "   macro avg       0.98      0.97      0.98       160\n",
      "weighted avg       0.98      0.98      0.98       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "adaboost = AdaBoostClassifier(n_estimators=10)\n",
    "\n",
    "gradboost = GradientBoostingClassifier(n_estimators=5)\n",
    "\n",
    "rforest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print('--- Decision Tree ---')\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=dtree, verbose=True)\n",
    "\n",
    "print('--- AdaBoost ---')\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=adaboost, verbose=True)\n",
    "\n",
    "print('--- Gradient Boosting ---')\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=gradboost, verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of performance, both the Gradient Boosting and AdaBoost classifiers achieve >99% validation accuracy and >94% test accuracy on the dataset, though there are some differences in the models. Gradient Boosting achieves this performance reliably, but training is an order of magnitude slower. In contrast, AdaBoost is less reliable, but trains significantly faster even with a larger number of estimators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Hanna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine\n",
      "Classifier fit in 1.484s\n",
      "Validation accuracy: 100.000%\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       1.00      1.00      1.00        61\n",
      "        COAD       1.00      1.00      1.00        19\n",
      "        KIRC       1.00      1.00      1.00        30\n",
      "        LUAD       1.00      1.00      1.00        19\n",
      "        PRAD       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           1.00       160\n",
      "   macro avg       1.00      1.00      1.00       160\n",
      "weighted avg       1.00      1.00      1.00       160\n",
      "\n",
      "Test Accuracy:  0.9937888198757764\n",
      "Classification Report (Test): \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       0.98      1.00      0.99        62\n",
      "        COAD       1.00      1.00      1.00        15\n",
      "        KIRC       1.00      0.97      0.98        30\n",
      "        LUAD       1.00      1.00      1.00        32\n",
      "        PRAD       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           0.99       161\n",
      "   macro avg       1.00      0.99      1.00       161\n",
      "weighted avg       0.99      0.99      0.99       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#initalize the SVM model\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "print(\"Support Vector Machine\")\n",
    "#evaluate the svm model\n",
    "val_acc = fit_and_val_sklearn_classifier(classifier=svm_model, verbose=True)\n",
    "\n",
    "#test the model on the test set\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_test = svm_model.predict(X_test)\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Classification Report (Test): \\n\", classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msai339",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
